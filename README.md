# Big-Data-Computing
Different Homeworks made for the Big Data Computing course

Each of the programs has tackled a different task, using as framework Apache Spark:
- The first one analyzes a file containing several points belongin to a set A or B. Through different operations two different Clustering Objective Functions are compared.
- The second one again takes a file containing several points, but this time doing a more fine-grained task: it performs a pipeline of operations in order to compute a so called "Fair Clusterin", typically used when points belong to different demographic groups. In this kind of task a Coreset technique was used
- The third one is an implementation of the Count-Min sketch and Count sketch algorithms. In this case a streaming pipeline was adopted, in order to process items coming from a server
